{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPwGyOr82d46CEcQ/Tqh91i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsandaver/essays/blob/main/PromptGenerator3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "movf-cbgtW5y"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk import pos_tag, word_tokenize\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "\n",
        "# Attempt to import Google Colab's files module; handle if not in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Download necessary NLTK data if not already available\n",
        "nltk_packages = ['wordnet', 'omw-1.4', 'punkt', 'averaged_perceptron_tagger', 'stopwords']\n",
        "for package in nltk_packages:\n",
        "    nltk.download(package, quiet=True)\n",
        "\n",
        "# Mapping NLTK POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"\n",
        "    Map POS tag to the format accepted by wordnet.synsets()\n",
        "    \"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None  # Return None if POS tag is not recognized\n",
        "\n",
        "# Function to load the dataset\n",
        "def load_dataset(default_file='your_dataset.jsonl'):\n",
        "    \"\"\"\n",
        "    Load dataset from a JSON Lines file.\n",
        "    If the file is not found locally, prompt the user to upload it (Colab only).\n",
        "    \"\"\"\n",
        "    if os.path.exists(default_file):\n",
        "        file_path = default_file\n",
        "    else:\n",
        "        if IN_COLAB:\n",
        "            logging.info(f\"File '{default_file}' not found locally. Please upload the dataset.\")\n",
        "            uploaded = files.upload()\n",
        "            if not uploaded:\n",
        "                raise FileNotFoundError(\"No file uploaded.\")\n",
        "            file_path = list(uploaded.keys())[0]\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"File '{default_file}' not found and not running in Colab.\")\n",
        "\n",
        "    dataset = []\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    try:\n",
        "                        dataset.append(json.loads(line))\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        logging.warning(f\"Error decoding JSON: {e} in line: {line}\")\n",
        "        if not dataset:\n",
        "            raise ValueError(\"Dataset is empty. Please check the file contents.\")\n",
        "        logging.info(f\"Loaded {len(dataset)} entries from the dataset.\")\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred while loading the dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Remove punctuation and tokenize the text into words.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if re.match(r'\\w+', token)]  # Keep words only\n",
        "    return tokens\n",
        "\n",
        "# Function to extract unique elements from the dataset\n",
        "def extract_elements_from_dataset(dataset):\n",
        "    \"\"\"\n",
        "    Extract unique elements from the dataset and define camera, film, and new creative elements.\n",
        "    \"\"\"\n",
        "    features = defaultdict(set)\n",
        "\n",
        "    # Define patterns for extraction (can be customized based on dataset)\n",
        "    patterns = {\n",
        "        'subjects': re.compile(r'\\b\\w+\\s+eyes\\b', re.IGNORECASE),\n",
        "        'settings': re.compile(r'\\bforest\\b|\\bocean\\b|\\bsea\\b|\\bcity\\b|\\bmountain\\b|\\bdesert\\b|\\bbeach\\b', re.IGNORECASE),\n",
        "        'moods': re.compile(r'\\bintrospective\\b|\\bserene\\b|\\bwistful\\b|\\breflective\\b|\\badventurous\\b|\\bjoyful\\b|\\bsomber\\b', re.IGNORECASE),\n",
        "        'lighting': re.compile(r'\\bsoft\\b|\\bgolden[-\\s]hour\\b|\\bsunset\\b|\\bmoonlight\\b|\\bneon\\b|\\bharsh\\b|\\bdim\\b', re.IGNORECASE),\n",
        "        'perspectives': re.compile(r'\\blow-angle\\b|\\bbug’s-eye\\b|\\bthree-quarter view\\b|\\bclose-up\\b|\\bwide-angle\\b|\\bbird’s-eye view\\b', re.IGNORECASE)\n",
        "    }\n",
        "\n",
        "    for entry in dataset:\n",
        "        prompt = entry.get('prompt', '')\n",
        "        if not prompt:\n",
        "            continue\n",
        "        for key, pattern in patterns.items():\n",
        "            matches = pattern.findall(prompt)\n",
        "            for match in matches:\n",
        "                features[key].add(match.lower())\n",
        "\n",
        "    # Define camera and film elements directly\n",
        "    features['camera'] = set([\n",
        "        'DSLR',\n",
        "        'mirrorless',\n",
        "        'film camera',\n",
        "        'medium format',\n",
        "        '35mm',\n",
        "        'Hasselblad'  # Added Hasselblad without \"camera\"\n",
        "    ])\n",
        "    features['film'] = set([\n",
        "        'Kodak Portra 400',\n",
        "        'Fujifilm Pro 400H',\n",
        "        'Ilford HP5 Plus',\n",
        "        'Kodak Tri-X 400',\n",
        "        'Cinestill 800T'\n",
        "    ])\n",
        "\n",
        "    # Additional creative elements\n",
        "    features['themes'] = set([\n",
        "        'cyberpunk',\n",
        "        'steampunk',\n",
        "        'noir',\n",
        "        'fantasy',\n",
        "        'sci-fi',\n",
        "        'surrealism',\n",
        "        'minimalism'\n",
        "    ])\n",
        "    features['styles'] = set([\n",
        "        'vintage',\n",
        "        'modern',\n",
        "        'abstract',\n",
        "        'realistic',\n",
        "        'impressionistic',\n",
        "        'expressionistic',\n",
        "        'geometric'\n",
        "    ])\n",
        "    features['colors'] = set([\n",
        "        'monochromatic',\n",
        "        'vibrant',\n",
        "        'pastel',\n",
        "        'neon',\n",
        "        'earth tones',\n",
        "        'complementary colors',\n",
        "        'analogous colors'\n",
        "    ])\n",
        "    features['abstract_concepts'] = set([\n",
        "        'time dilation',\n",
        "        'metamorphosis',\n",
        "        'juxtaposition',\n",
        "        'paradox',\n",
        "        'transcendence',\n",
        "        'chaos',\n",
        "        'harmony'\n",
        "    ])\n",
        "\n",
        "    # Convert all sets to lists\n",
        "    for key in features:\n",
        "        features[key] = list(features[key])\n",
        "\n",
        "    logging.info(\"Extracted additional elements from the dataset.\")\n",
        "    return features\n",
        "\n",
        "# Function to combine base prompt with additional elements using dynamic structures\n",
        "def combine_elements(base_prompt, additional_elements):\n",
        "    \"\"\"\n",
        "    Incorporate additional elements into the base prompt to create a more detailed and creative prompt.\n",
        "    Uses dynamic sentence structures for variety.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        parts = [base_prompt]\n",
        "        element_keys = ['subjects', 'settings', 'themes', 'styles', 'colors', 'moods', 'lighting', 'perspectives', 'abstract_concepts']\n",
        "        selected_elements = {}\n",
        "\n",
        "        for key in element_keys:\n",
        "            if additional_elements.get(key):\n",
        "                selected_elements[key] = random.choice(additional_elements[key])\n",
        "\n",
        "        # Dynamic sentence structures\n",
        "        structures = [\n",
        "            \"{base} Featuring {subjects}, set in a {settings} with a {moods} mood.\",\n",
        "            \"{base}. A {styles} interpretation with {colors} hues and {lighting} lighting.\",\n",
        "            \"{base} captured from a {perspectives} perspective, embodying {abstract_concepts}.\",\n",
        "            \"{base} in a {themes} style, highlighting {subjects} against a {settings} backdrop.\"\n",
        "        ]\n",
        "\n",
        "        structure = random.choice(structures)\n",
        "        filled_structure = structure.format(\n",
        "            base=base_prompt,\n",
        "            subjects=selected_elements.get('subjects', ''),\n",
        "            settings=selected_elements.get('settings', ''),\n",
        "            moods=selected_elements.get('moods', ''),\n",
        "            styles=selected_elements.get('styles', ''),\n",
        "            colors=selected_elements.get('colors', ''),\n",
        "            lighting=selected_elements.get('lighting', ''),\n",
        "            perspectives=selected_elements.get('perspectives', ''),\n",
        "            abstract_concepts=selected_elements.get('abstract_concepts', ''),\n",
        "            themes=selected_elements.get('themes', '')\n",
        "        )\n",
        "\n",
        "        # Incorporate camera and film elements\n",
        "        camera_film_sentence = \"\"\n",
        "        if additional_elements.get('camera') or additional_elements.get('film'):\n",
        "            camera = random.choice(additional_elements['camera']) if additional_elements.get('camera') else ''\n",
        "            film = random.choice(additional_elements['film']) if additional_elements.get('film') else ''\n",
        "            camera_film_sentence = \" Captured using a {}{}.\".format(\n",
        "                camera,\n",
        "                f\" on {film} film\" if film else \"\"\n",
        "            )\n",
        "        filled_structure += camera_film_sentence\n",
        "\n",
        "        return filled_structure\n",
        "    except KeyError as e:\n",
        "        logging.warning(f\"Missing feature category: {e}\")\n",
        "        return base_prompt\n",
        "\n",
        "# Cache for synonyms to improve performance\n",
        "synonym_cache = {}\n",
        "\n",
        "# Function to find and replace words with synonyms for added creativity\n",
        "def replace_with_synonyms(prompt, protected_words):\n",
        "    \"\"\"\n",
        "    Replace adjectives, adverbs, and nouns in the prompt with their synonyms to add variety, excluding protected words.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(prompt)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    new_tokens = []\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Split protected words into tokens\n",
        "    protected_tokens = set()\n",
        "    for word in protected_words:\n",
        "        protected_tokens.update(word_tokenize(word.lower()))\n",
        "\n",
        "    for word, tag in tagged_tokens:\n",
        "        wn_pos = get_wordnet_pos(tag)\n",
        "        # Replace adjectives, adverbs, and nouns for more creativity\n",
        "        if wn_pos not in (wordnet.ADJ, wordnet.ADV, wordnet.NOUN):\n",
        "            new_tokens.append(word)\n",
        "            continue\n",
        "\n",
        "        # Exclude stopwords, function words, and protected words\n",
        "        if word.lower() in stop_words or word.lower() in protected_tokens:\n",
        "            new_tokens.append(word)\n",
        "            continue\n",
        "\n",
        "        word_lower = word.lower()\n",
        "        # Check cache first\n",
        "        if word_lower in synonym_cache:\n",
        "            synonyms = synonym_cache[word_lower]\n",
        "        else:\n",
        "            synsets = wordnet.synsets(word_lower, pos=wn_pos)\n",
        "            synonyms = set()\n",
        "            for syn in synsets:\n",
        "                for lemma in syn.lemmas():\n",
        "                    syn_name = lemma.name().replace('_', ' ')\n",
        "                    # Include multi-word synonyms\n",
        "                    if syn_name.isalpha() or ' ' in syn_name:\n",
        "                        synonyms.add(syn_name)\n",
        "            # Filter to alphabetic synonyms\n",
        "            synonyms = {syn for syn in synonyms if syn.replace(' ', '').isalpha()}\n",
        "            synonym_cache[word_lower] = list(synonyms)\n",
        "\n",
        "        # Exclude the original word and select a random synonym\n",
        "        synonyms = [syn for syn in synonym_cache[word_lower] if syn.lower() != word_lower]\n",
        "        if synonyms:\n",
        "            synonym = random.choice(synonyms)\n",
        "            # Preserve the original casing\n",
        "            if word.isupper():\n",
        "                synonym = synonym.upper()\n",
        "            elif word[0].isupper():\n",
        "                synonym = synonym.capitalize()\n",
        "            new_tokens.append(synonym)\n",
        "        else:\n",
        "            new_tokens.append(word)\n",
        "\n",
        "    return ' '.join(new_tokens)\n",
        "\n",
        "# Function to truncate the prompt to a given token limit\n",
        "def truncate_prompt(prompt, token_limit):\n",
        "    \"\"\"\n",
        "    Truncate the prompt to the specified number of tokens without breaking words.\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(prompt)\n",
        "    if len(tokens) <= token_limit:\n",
        "        return prompt\n",
        "    truncated_tokens = tokens[:token_limit]\n",
        "    # Ensure that the prompt ends gracefully\n",
        "    truncated_prompt = ' '.join(truncated_tokens)\n",
        "    if not truncated_prompt.endswith(('.', '!', '?')):\n",
        "        truncated_prompt += '...'\n",
        "    return truncated_prompt\n",
        "\n",
        "# Function to generate a creative random prompt based on dataset elements\n",
        "def generate_prompt(dataset, additional_elements, token_limit=75):\n",
        "    \"\"\"\n",
        "    Generate a creative prompt by combining elements from the dataset and adding synonyms.\n",
        "    \"\"\"\n",
        "    base_entry = random.choice(dataset)\n",
        "    base_prompt = base_entry.get('prompt', 'A creative scene.')\n",
        "\n",
        "    combined_prompt = combine_elements(base_prompt, additional_elements)\n",
        "\n",
        "    # Combine camera and film elements to protect them from synonym replacement\n",
        "    protected_words = set(\n",
        "        word.lower() for word in additional_elements.get('camera', []) + additional_elements.get('film', [])\n",
        "    )\n",
        "\n",
        "    combined_prompt = replace_with_synonyms(combined_prompt, protected_words)\n",
        "\n",
        "    if len(tokenize(combined_prompt)) > token_limit:\n",
        "        combined_prompt = truncate_prompt(combined_prompt, token_limit)\n",
        "\n",
        "    return combined_prompt\n",
        "\n",
        "# Function to save the generated prompts to a file\n",
        "def save_generated_prompts(prompts, output_file):\n",
        "    \"\"\"\n",
        "    Save the list of generated prompts to a text file, each on a new line.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8') as file:\n",
        "            for prompt in prompts:\n",
        "                file.write(f\"{prompt}\\n\")\n",
        "        logging.info(f\"{len(prompts)} prompts generated and saved to '{output_file}'.\")\n",
        "\n",
        "        if IN_COLAB:\n",
        "            files.download(output_file)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred while saving prompts: {e}\")\n",
        "        raise\n",
        "\n",
        "# Main function to run the generator\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the prompt generation process.\n",
        "    \"\"\"\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Generate creative prompts based on a dataset.\")\n",
        "    parser.add_argument('--dataset', type=str, default='your_dataset.jsonl', help='Path to the dataset JSONL file.')\n",
        "    parser.add_argument('--output', type=str, default='generated_prompts.txt', help='Output file for generated prompts.')\n",
        "    parser.add_argument('--number', type=int, default=10, help='Number of prompts to generate.')\n",
        "    parser.add_argument('--tokens', type=int, default=75, help='Maximum number of tokens per prompt.')\n",
        "    parser.add_argument('--categories', type=str, nargs='*', default=[], help='Categories to include (e.g., themes styles). If empty, all are included.')\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    try:\n",
        "        dataset = load_dataset(args.dataset)\n",
        "    except Exception as e:\n",
        "        logging.error(e)\n",
        "        return\n",
        "\n",
        "    additional_elements = extract_elements_from_dataset(dataset)\n",
        "\n",
        "    # If user specifies categories, filter them\n",
        "    if args.categories:\n",
        "        allowed_categories = set(args.categories)\n",
        "        additional_elements = {k: v for k, v in additional_elements.items() if k in allowed_categories}\n",
        "        logging.info(f\"Selected categories for prompt generation: {', '.join(allowed_categories)}\")\n",
        "    else:\n",
        "        logging.info(\"Using all available categories for prompt generation.\")\n",
        "\n",
        "    if not any(additional_elements.values()):\n",
        "        logging.error(\"No additional elements extracted. Please check the dataset's 'prompt' fields.\")\n",
        "        return\n",
        "\n",
        "    generated_prompts = []\n",
        "    seen_prompts = set()\n",
        "\n",
        "    for _ in range(args.number):\n",
        "        attempt = 0\n",
        "        max_attempts = 5\n",
        "        while attempt < max_attempts:\n",
        "            generated_prompt = generate_prompt(dataset, additional_elements, token_limit=args.tokens)\n",
        "            if generated_prompt not in seen_prompts:\n",
        "                generated_prompts.append(generated_prompt)\n",
        "                seen_prompts.add(generated_prompt)\n",
        "                break\n",
        "            attempt += 1\n",
        "        else:\n",
        "            logging.warning(\"Max attempts reached. Some prompts may be duplicates.\")\n",
        "\n",
        "    try:\n",
        "        save_generated_prompts(generated_prompts, args.output)\n",
        "    except Exception as e:\n",
        "        logging.error(e)\n",
        "        return\n",
        "\n",
        "# Entry point\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}