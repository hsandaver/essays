{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN4dWeIsvAB8v24GVTTAdn7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsandaver/essays/blob/main/entity_extractor_wikipediav1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM-CldL8H1d7"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", package])\n",
        "\n",
        "# Install required packages\n",
        "install(\"pymupdf\")\n",
        "install(\"spacy\")\n",
        "install(\"SPARQLWrapper\")\n",
        "install(\"pandas\")\n",
        "\n",
        "# Download the Spacy model if not already present\n",
        "import spacy\n",
        "\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "from google.colab import files\n",
        "from IPython.display import display  # For displaying DataFrame\n",
        "\n",
        "# Function to upload files in Colab\n",
        "def upload_pdf():\n",
        "    print(\"Please upload your PDF file.\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded. Exiting.\")\n",
        "        sys.exit()\n",
        "    pdf_path = next(iter(uploaded))\n",
        "    return pdf_path\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(doc):\n",
        "    text = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)  # Load page\n",
        "        text += page.get_text()  # Extract text from page\n",
        "    return text\n",
        "\n",
        "# Function to perform entity recognition\n",
        "def extract_person_entities(text, nlp_model):\n",
        "    doc_nlp = nlp_model(text)\n",
        "    person_entities = [ent.text.strip() for ent in doc_nlp.ents if ent.label_ == \"PERSON\"]\n",
        "    # Remove duplicates and short names which might be false positives\n",
        "    person_entities = list(set([ent for ent in person_entities if len(ent) > 1]))\n",
        "    return person_entities\n",
        "\n",
        "# Function to query Wikidata using SPARQL\n",
        "def query_wikidata(entity):\n",
        "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "    query = f\"\"\"\n",
        "    SELECT ?item ?itemLabel ?itemDescription WHERE {{\n",
        "      ?item rdfs:label \"{entity}\"@en.\n",
        "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
        "    }} LIMIT 1\n",
        "    \"\"\"\n",
        "    sparql.setQuery(query)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    try:\n",
        "        results = sparql.query().convert()\n",
        "        bindings = results[\"results\"][\"bindings\"]\n",
        "        if bindings:\n",
        "            item = bindings[0][\"item\"][\"value\"]\n",
        "            item_id = item.split('/')[-1]\n",
        "            label = bindings[0].get(\"itemLabel\", {}).get(\"value\", \"N/A\")\n",
        "            description = bindings[0].get(\"itemDescription\", {}).get(\"value\", \"N/A\")\n",
        "            return item_id, label, description\n",
        "        else:\n",
        "            return None, None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Wikidata for entity '{entity}': {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Main processing function\n",
        "def process_pdf():\n",
        "    pdf_path = upload_pdf()\n",
        "    print(f\"Processing PDF: {pdf_path}\")\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening PDF: {e}\")\n",
        "        sys.exit()\n",
        "\n",
        "    pdf_text = extract_text_from_pdf(doc)\n",
        "    print(\"Extracted text from PDF.\")\n",
        "\n",
        "    person_entities = extract_person_entities(pdf_text, nlp)\n",
        "    print(f\"Found {len(person_entities)} unique person entities.\")\n",
        "\n",
        "    if not person_entities:\n",
        "        print(\"No person entities found in the PDF.\")\n",
        "        sys.exit()\n",
        "\n",
        "    # Query Wikidata for each person entity and collect data\n",
        "    entity_data = []\n",
        "    for idx, entity in enumerate(person_entities, 1):\n",
        "        print(f\"Querying Wikidata for entity {idx}/{len(person_entities)}: {entity}\")\n",
        "        entity_id, label, description = query_wikidata(entity)\n",
        "        if entity_id:\n",
        "            entity_data.append({\n",
        "                \"Name\": entity,\n",
        "                \"Wikidata ID\": entity_id,\n",
        "                \"Label\": label,\n",
        "                \"Description\": description\n",
        "            })\n",
        "        else:\n",
        "            entity_data.append({\n",
        "                \"Name\": entity,\n",
        "                \"Wikidata ID\": \"N/A\",\n",
        "                \"Label\": \"N/A\",\n",
        "                \"Description\": \"N/A\"\n",
        "            })\n",
        "\n",
        "    # Convert the results to a pandas DataFrame for display\n",
        "    df = pd.DataFrame(entity_data)\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(\"\\n=== Person Entities Extracted ===\")\n",
        "    display(df)\n",
        "\n",
        "    # Optionally, allow the user to download the DataFrame as a CSV\n",
        "    try:\n",
        "        csv = df.to_csv(index=False)\n",
        "        with open(\"person_entities.csv\", \"w\") as f:\n",
        "            f.write(csv)\n",
        "        print(\"\\nDownloading 'person_entities.csv'...\")\n",
        "        files.download('person_entities.csv')\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading CSV: {e}\")\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    process_pdf()"
      ]
    }
  ]
}